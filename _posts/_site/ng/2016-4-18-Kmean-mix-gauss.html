<h2 id="k">K均值聚类算法</h2>
<p>在聚类问题中，我们给定一个训练集<script type="math/tex">\{x^{(1)},\cdots,x^{(m)}\}</script>，标签<script type="math/tex">y^{(i)}</script>没有给出，我们的目标还是把数据分类。这是一个非监督学习问题。</p>

<p>K均值聚类算法给出了下面的方法：</p>

<ol>
  <li>随机初始化聚类中心<script type="math/tex">\mu_1,\mu_2,\cdots,\mu_k \in \mathbb{R}^n</script></li>
  <li>
    <p>不断迭代直到收敛：{</p>

    <p>对所有i，令：</p>

    <script type="math/tex; mode=display">c^{(i)} := \arg \min_j\|x^{(i)}-\mu_j\|^2</script>

    <p>对所有j，令：</p>

    <script type="math/tex; mode=display">\mu_j := \frac{\sum_{i=1}^m 1\{c^{(i)}=j\}x^{(i)}}{\sum_{i=1}^m 1\{c^{(i)}=j\}}</script>

    <p>}</p>
  </li>
</ol>

<p>算法的内循环不停的在执行两个步骤，先把每个训练样本归给最近的聚类中心所代表的类，然后将某类的所有点计算平均值作为新的聚类中心。</p>

<p>现在有一个问题，如何保证K均值聚类算法一定会收敛？定义失真函数为：</p>

<script type="math/tex; mode=display">J(c,\mu)=\sum_{i=1}^m\|x^{(i)}-\mu_{c^{(i)}}\|^2</script>

<p>失真函数定义了所有训练集离其聚类中心平方距离和。对于算法的内循环的两步而言，每一步都是朝着J下降的方向进行。实际上K均值聚类算法就是一类坐标下降法。最终J会达到收敛。</p>

<p>失真函数J是一个非凸函数，所以坐标下降法不能保证J能收敛到全局最小值。为了不限于局部最小值，可以多次跑K均值聚类算法（使用不同的随机初始值），取最低失真函数。</p>

<h2 id="section">混合高斯模型</h2>
<p>同样假设下面的非监督学习问题，给定一组训练集<script type="math/tex">\{x^{(i)},\cdots,x^{(m)}\}</script>，没有标签。</p>

<p>我们用一个联合分布<script type="math/tex">p(x^{(i)},z^{(i)})=p(x^{(i)} \mid z^{(i)})p(z^{(i)})</script>来对数据进行建模。这里，<script type="math/tex">z^{(i)} \sim \text{Multinomial}(\phi)（\phi_j \geq 0, \sum_{j=1}^k \phi_j = 1, \phi_j=p(z^{(i)}=j))</script>，<script type="math/tex">x^{(i)} \mid z^{(i)}=j \sim \mathcal{N}(\mu_j,\Sigma_j)</script>。因此我们的模型是随机从<script type="math/tex">\{1,\cdots,k\}</script>中选择<script type="math/tex">z^{(i)}</script>，然后从对应<script type="math/tex">z^{(i)}</script>的高斯分布中选择<script type="math/tex">x^{(i)}</script>。这被称为混合高斯模型。此外由于<script type="math/tex">z^{(i)}</script>是潜在的随机变量，这使我们的评估变得困难。</p>

<p>混合高斯模型的参数有<script type="math/tex">\phi,\mu,\Sigma</script>，最大似然函数是：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
\ell(\phi,\mu,\Sigma) &= \sum_{i=1}^m \log p(x^{(i)};\phi,\mu,\Sigma) \\
&= \sum_{i=1}^m \log \sum_{z^{(i)}=1}^k p(x^{(i)}\mid z^{(i)};\mu,\Sigma)p(z^{(i)};\phi)
\end{align}
 %]]></script>

<p>然而上式无法用梯度来计算，因此也无法解得最大似然估计。</p>

<p>注意上式中<script type="math/tex">\sum_{z^{(i)}=1}^k</script>产生的原因在于<script type="math/tex">z^{(i)}</script>是未知的，假如<script type="math/tex">z^{(i)}</script>是已知的，那么最大似然函数可以简化为：</p>

<script type="math/tex; mode=display">\ell(\phi,\mu,\Sigma)=\sum_{i=1}^m\log p(x^{(i)}\mid z^{(i)};\mu,\Sigma)+p(z^{(i)};\phi)</script>

<p>做最大似然估计可以解出：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
\phi_j &= \frac{1}{m} \sum_{i=1}^m 1 \{z^{(i)}=j\}  \\
\mu_j &= \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}x^{(i)}}{\sum_{i=1}^m 1\{z^{(i)}=j\}} \\
\Sigma_j &= \frac{\sum_{i=1}^m 1\{z^{(i)}=j\}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m 1\{z^{(i)}=j\}}
\end{align}
 %]]></script>

<p>事实上，假如<script type="math/tex">z^{(i)}</script>是已知的，那么最大似然估计和之前的高斯判别分析非常类似，只是<script type="math/tex">z^{(i)}</script>表示了标签。</p>

<p>然而<script type="math/tex">z^{(i)}</script>是未知的，这里就引出了最大期望算法。最大期望算法主要由两步构成，E步尝试猜测<script type="math/tex">z^{(i)}</script>的值，M步根据我们猜测的<script type="math/tex">z^{(i)}</script>更新模型参数。</p>

<p>迭代直到收敛：{</p>

<p>（E步）对每个i,j，令：</p>

<script type="math/tex; mode=display">\omega_j^{(i)} := p(z^{(i)}=j\mid x^{(i)}; \phi, \mu, \Sigma)</script>

<p>（M步）更新参数：</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
\phi_j &:= \frac{1}{m} \sum_{i=1}^m \omega_j^{(i)}  \\
\mu_j &:= \frac{\sum_{i=1}^m \omega_j^{(i)}x^{(i)}}{\sum_{i=1}^m \omega_j^{(i)}} \\
\Sigma_j &= \frac{\sum_{i=1}^m \omega_j^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^m \omega_j^{(i)}}
\end{align}
 %]]></script>

<p>}</p>

<p>在E步，我们通过计算后验概率来得到：</p>

<script type="math/tex; mode=display">p(z^{(i)}=j\mid x^{(i)}; \phi, \mu, \Sigma)=\frac{p(x^{(i)}|z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j;\phi)}{\sum_{l=1}^kp(x^{(i)}|z^{(i)}=l;\mu,\Sigma)p(z^{(i)}=l;\phi)}</script>

<p>EM算法和K均值聚类算法有很多相似的地方，都是两步走，都是可能陷入局部最优解，需要多次不同的初始值进行计算。</p>

<p>EM算法是否确保收敛等性质放到下一章证明。</p>
